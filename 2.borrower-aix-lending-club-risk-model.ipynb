{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibmdbpy import IdaDataBase, IdaDataFrame\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_1 = {\n",
    "    'username': 'username',\n",
    "    'password': \"\"\"password\"\"\",\n",
    "    'sg_service_url': 'url',\n",
    "    'database': 'BLUDB',\n",
    "    'host': 'host',\n",
    "    'port': 'port',\n",
    "    'auto_discovery': 'true',\n",
    "}\n",
    "df = IdaDataFrame(idadb_c568b279819b490d915961aba3c756c5, 'BXJ33585.LENDINGCLUB').as_dataframe()\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <th>total_bal_ex_mort</th>\n",
       "      <th>total_bc_limit</th>\n",
       "      <th>total_il_high_credit_limit</th>\n",
       "      <th>hardship_flag</th>\n",
       "      <th>disbursement_method</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>grade_predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68355089</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>24700.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>11.99</td>\n",
       "      <td>820.28</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>314017.0</td>\n",
       "      <td>39475.0</td>\n",
       "      <td>79300.0</td>\n",
       "      <td>24667.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68476807</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>22.45</td>\n",
       "      <td>289.91</td>\n",
       "      <td>F</td>\n",
       "      <td>F1</td>\n",
       "      <td>3 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>439570.0</td>\n",
       "      <td>95768.0</td>\n",
       "      <td>20300.0</td>\n",
       "      <td>88097.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68446771</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.77</td>\n",
       "      <td>252.32</td>\n",
       "      <td>D</td>\n",
       "      <td>D1</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255047.0</td>\n",
       "      <td>275831.0</td>\n",
       "      <td>6700.0</td>\n",
       "      <td>246447.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68376217</td>\n",
       "      <td>23100.0</td>\n",
       "      <td>23100.0</td>\n",
       "      <td>23100.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>20.50</td>\n",
       "      <td>618.46</td>\n",
       "      <td>E</td>\n",
       "      <td>E4</td>\n",
       "      <td>2 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102524.0</td>\n",
       "      <td>55048.0</td>\n",
       "      <td>21700.0</td>\n",
       "      <td>80824.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68446746</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>5.32</td>\n",
       "      <td>361.38</td>\n",
       "      <td>A</td>\n",
       "      <td>A1</td>\n",
       "      <td>4 years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183062.0</td>\n",
       "      <td>11146.0</td>\n",
       "      <td>8500.0</td>\n",
       "      <td>29053.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Cash</td>\n",
       "      <td>N</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  loan_amnt  funded_amnt  funded_amnt_inv        term  int_rate  \\\n",
       "0  68355089    24700.0      24700.0          24700.0   36 months     11.99   \n",
       "1  68476807    10400.0      10400.0          10400.0   60 months     22.45   \n",
       "2  68446771     7200.0       7200.0           7200.0   36 months     15.77   \n",
       "3  68376217    23100.0      23100.0          23100.0   60 months     20.50   \n",
       "4  68446746    12000.0      12000.0          12000.0   36 months      5.32   \n",
       "\n",
       "   installment grade sub_grade emp_length  ... pub_rec_bankruptcies  \\\n",
       "0       820.28     C        C1  10+ years  ...                  0.0   \n",
       "1       289.91     F        F1    3 years  ...                  0.0   \n",
       "2       252.32     D        D1   < 1 year  ...                  0.0   \n",
       "3       618.46     E        E4    2 years  ...                  0.0   \n",
       "4       361.38     A        A1    4 years  ...                  0.0   \n",
       "\n",
       "   tax_liens tot_hi_cred_lim total_bal_ex_mort total_bc_limit  \\\n",
       "0        0.0        314017.0           39475.0        79300.0   \n",
       "1        0.0        439570.0           95768.0        20300.0   \n",
       "2        1.0        255047.0          275831.0         6700.0   \n",
       "3        0.0        102524.0           55048.0        21700.0   \n",
       "4        0.0        183062.0           11146.0         8500.0   \n",
       "\n",
       "  total_il_high_credit_limit hardship_flag disbursement_method  \\\n",
       "0                    24667.0             N                Cash   \n",
       "1                    88097.0             N                Cash   \n",
       "2                   246447.0             N                Cash   \n",
       "3                    80824.0             N                Cash   \n",
       "4                    29053.0             N                Cash   \n",
       "\n",
       "  debt_settlement_flag  grade_predicted  \n",
       "0                    N                C  \n",
       "1                    N                G  \n",
       "2                    N                B  \n",
       "3                    N                G  \n",
       "4                    N                A  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['id','application_type','loan_amnt','purpose','annual_inc','fico_range_low','term','dti','revol_bal','revol_util',\n",
    "           'inq_last_6mths','delinq_2yrs','pub_rec','total_bal_il','total_rev_hi_lim','avg_cur_bal','tot_hi_cred_lim', \n",
    "           'grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df2.drop('grade', axis=1), df2['grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_purpose(value):\n",
    "    if value in ['debt_consolidation', 'credit_card','medical', 'house', 'car', 'small_business', 'other']:\n",
    "        return value\n",
    "    else:\n",
    "        return 'personal'\n",
    "\n",
    "def transform_application_type(value):\n",
    "    if str(value).strip() == 'individual':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def transform_term(value):\n",
    "    value = value.split()[0]\n",
    "    return int(value)\n",
    "\n",
    "\n",
    "X_resampled['purpose'] = X_resampled['purpose'].apply(transform_purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(value):\n",
    "    return str(value).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_resampled.select_dtypes('object').columns:\n",
    "    X_resampled[i] = X_resampled[i].apply(strip) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Training Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_resampled = le.fit_transform(y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, stratify=y_resampled, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reindex = X_train.reset_index().drop('index', axis=1)\n",
    "X_test_reindex = X_test.reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohePurpose = OneHotEncoder()\n",
    "X_trainOHE = ohePurpose.fit_transform(X_train[['purpose']])\n",
    "X_testOHE = ohePurpose.transform(X_test[['purpose']])\n",
    "\n",
    "X_trainOHE = pd.DataFrame(X_trainOHE.toarray(), columns=ohePurpose.categories_)\n",
    "X_testOHE = pd.DataFrame(X_testOHE.toarray(), columns=ohePurpose.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainOHE.columns = ['_'.join(col) for col in X_trainOHE.columns]\n",
    "X_testOHE.columns = ['_'.join(col) for col in X_testOHE.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainFinal = pd.concat([X_train_reindex, X_trainOHE], axis=1).drop('purpose', axis=1)\n",
    "X_testFinal = pd.concat([X_test_reindex, X_testOHE], axis=1).drop('purpose', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainFinal['application_type'] = X_trainFinal['application_type'].apply(transform_application_type)\n",
    "X_testFinal['application_type'] = X_testFinal['application_type'].apply(transform_application_type)\n",
    "X_trainFinal['term'] = X_trainFinal['term'].apply(transform_term)\n",
    "X_testFinal['term'] = X_testFinal['term'].apply(transform_term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What could be the rules that differentiate between grade A and B?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aix360.algorithms.rbm import FeatureBinarizer\n",
    "fb = FeatureBinarizer(negations=True, returnOrd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train.copy()\n",
    "train['grade'] = le.inverse_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_test.copy()\n",
    "test['grade'] = le.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAB = train[train['grade'].isin(['A','B'])]\n",
    "testAB = test[test['grade'].isin(['A','B'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempLE = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainAB = trainAB.drop('grade', axis=1)\n",
    "y_trainAB = tempLE.fit_transform(trainAB['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testAB = testAB.drop('grade', axis=1)\n",
    "y_testAB = tempLE.transform(testAB['grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainAB_Save = X_trainAB.copy()\n",
    "X_testAB_Save = X_testAB.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainAB, X_trainStd = fb.fit_transform(X_trainAB)\n",
    "X_testAB, X_testStd = fb.transform(X_testAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aix360.algorithms.rbm import BooleanRuleCG\n",
    "br = BooleanRuleCG(lambda0 = 0.001, lambda1 = 0.001, CNF=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempLE.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Using Boolean Rule Column Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning CNF rule with complexity parameters lambda0=0.001, lambda1=0.001\n",
      "Initial LP solved\n",
      "Iteration: 1, Objective: 0.3221\n",
      "Iteration: 2, Objective: 0.2811\n",
      "Iteration: 3, Objective: 0.2811\n",
      "Iteration: 4, Objective: 0.2811\n",
      "Iteration: 5, Objective: 0.2786\n",
      "Iteration: 6, Objective: 0.2786\n",
      "Iteration: 7, Objective: 0.2786\n",
      "Training accuracy: 0.7243675099866844\n",
      "Test accuracy: 0.7152847152847153\n",
      "Predict Y=0 [A] if ANY of the following rules are satisfied, otherwise Y=1 [B]:\n",
      "['fico_range_low > 700.00 AND term not ']\n"
     ]
    }
   ],
   "source": [
    "br.fit(X_trainAB, y_trainAB)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training accuracy:', accuracy_score(y_trainAB, br.predict(X_trainAB)))\n",
    "print('Test accuracy:', accuracy_score(y_testAB, br.predict(X_testAB)))\n",
    "print('Predict Y=0 [A] if ANY of the following rules are satisfied, otherwise Y=1 [B]:')\n",
    "print(br.explain()['rules'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rules in here meaning that:\n",
    "\n",
    "```If your fico score more than 700 and Term is 36 months, then you're grade A, else B```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Logistic Rule Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ariff/.local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/jupyter-ariff/.local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/jupyter-ariff/.local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/jupyter-ariff/.local/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.747503328894807\n",
      "Test accuracy: 0.7357642357642358\n",
      "Probability of Y=1 is predicted as logistic(z) = 1 / (1 + exp(-z))\n",
      "where z is a linear combination of the following rules/numerical features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule/numerical feature</th>\n",
       "      <th>coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(intercept)</td>\n",
       "      <td>0.467627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>term</td>\n",
       "      <td>1.80864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>term not</td>\n",
       "      <td>-1.41895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fico_range_low</td>\n",
       "      <td>-1.37287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>application_type not  AND purpose != house AND...</td>\n",
       "      <td>-1.05526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>purpose != house AND purpose != medical AND pu...</td>\n",
       "      <td>-0.809793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id &lt;= 105042485.80</td>\n",
       "      <td>-0.49988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tot_hi_cred_lim &lt;= 68908.30</td>\n",
       "      <td>0.496476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fico_range_low &lt;= 710.00</td>\n",
       "      <td>0.492631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id &lt;= 126356403.40</td>\n",
       "      <td>0.477911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>purpose == credit_card</td>\n",
       "      <td>-0.473912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>id &lt;= 136459060.40</td>\n",
       "      <td>0.470564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fico_range_low &lt;= 680.00</td>\n",
       "      <td>0.443648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>inq_last_6mths</td>\n",
       "      <td>0.395695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>total_rev_hi_lim &lt;= 31800.00</td>\n",
       "      <td>0.359621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>total_rev_hi_lim &lt;= 16100.00</td>\n",
       "      <td>0.342422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>purpose != house AND purpose != medical AND pu...</td>\n",
       "      <td>0.306534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>revol_util</td>\n",
       "      <td>0.304982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dti &lt;= 24.35</td>\n",
       "      <td>-0.30482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dti &lt;= 11.77</td>\n",
       "      <td>-0.304498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>total_rev_hi_lim &lt;= 60600.00</td>\n",
       "      <td>0.287579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>fico_range_low &lt;= 670.00</td>\n",
       "      <td>0.28123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>inq_last_6mths &lt;= 0.00</td>\n",
       "      <td>-0.250404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>revol_util &lt;= 53.30</td>\n",
       "      <td>-0.237289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>avg_cur_bal &lt;= 12472.80</td>\n",
       "      <td>0.194618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dti &lt;= 21.06</td>\n",
       "      <td>-0.180854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>fico_range_low &lt;= 700.00</td>\n",
       "      <td>0.175349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>total_rev_hi_lim &lt;= 20855.00</td>\n",
       "      <td>0.171171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>purpose != house AND purpose != medical AND pu...</td>\n",
       "      <td>-0.158383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>annual_inc &lt;= 60000.00</td>\n",
       "      <td>0.0983166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>id &lt;= 129097456.00</td>\n",
       "      <td>0.0795461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>dti</td>\n",
       "      <td>-0.0565311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tot_hi_cred_lim &lt;= 97678.00</td>\n",
       "      <td>-0.0316408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>total_rev_hi_lim &lt;= 38900.00</td>\n",
       "      <td>0.0111804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>total_rev_hi_lim</td>\n",
       "      <td>-0.00954004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               rule/numerical feature coefficient\n",
       "0                                         (intercept)    0.467627\n",
       "1                                              term       1.80864\n",
       "2                                           term not     -1.41895\n",
       "3                                      fico_range_low    -1.37287\n",
       "4   application_type not  AND purpose != house AND...    -1.05526\n",
       "5   purpose != house AND purpose != medical AND pu...   -0.809793\n",
       "6                                  id <= 105042485.80    -0.49988\n",
       "7                         tot_hi_cred_lim <= 68908.30    0.496476\n",
       "8                            fico_range_low <= 710.00    0.492631\n",
       "9                                  id <= 126356403.40    0.477911\n",
       "10                             purpose == credit_card   -0.473912\n",
       "11                                 id <= 136459060.40    0.470564\n",
       "12                           fico_range_low <= 680.00    0.443648\n",
       "13                                     inq_last_6mths    0.395695\n",
       "14                       total_rev_hi_lim <= 31800.00    0.359621\n",
       "15                       total_rev_hi_lim <= 16100.00    0.342422\n",
       "16  purpose != house AND purpose != medical AND pu...    0.306534\n",
       "17                                         revol_util    0.304982\n",
       "18                                       dti <= 24.35    -0.30482\n",
       "19                                       dti <= 11.77   -0.304498\n",
       "20                       total_rev_hi_lim <= 60600.00    0.287579\n",
       "21                           fico_range_low <= 670.00     0.28123\n",
       "22                             inq_last_6mths <= 0.00   -0.250404\n",
       "23                                revol_util <= 53.30   -0.237289\n",
       "24                            avg_cur_bal <= 12472.80    0.194618\n",
       "25                                       dti <= 21.06   -0.180854\n",
       "26                           fico_range_low <= 700.00    0.175349\n",
       "27                       total_rev_hi_lim <= 20855.00    0.171171\n",
       "28  purpose != house AND purpose != medical AND pu...   -0.158383\n",
       "29                             annual_inc <= 60000.00   0.0983166\n",
       "30                                 id <= 129097456.00   0.0795461\n",
       "31                                                dti  -0.0565311\n",
       "32                        tot_hi_cred_lim <= 97678.00  -0.0316408\n",
       "33                       total_rev_hi_lim <= 38900.00   0.0111804\n",
       "34                                   total_rev_hi_lim -0.00954004"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aix360.algorithms.rbm import LogisticRuleRegression\n",
    "lrr = LogisticRuleRegression(lambda0=0.005, lambda1=0.001, useOrd=True)\n",
    "\n",
    "# Train, print, and evaluate model\n",
    "lrr.fit(X_trainAB, y_trainAB, X_trainStd)\n",
    "print('Training accuracy:', accuracy_score(y_trainAB, lrr.predict(X_trainAB, X_trainStd)))\n",
    "print('Test accuracy:', accuracy_score(y_testAB, lrr.predict(X_testAB, X_testStd)))\n",
    "print('Probability of Y=1 is predicted as logistic(z) = 1 / (1 + exp(-z))')\n",
    "print('where z is a linear combination of the following rules/numerical features:')\n",
    "lrr.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As your FICO score higher, your chances to be in grade A also increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrr.visualize(X, fb, ['fico_range_low']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you take long term , your chances to be in grade A also decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrr.visualize(X, fb, ['term']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As your revolving line utilization rate is higher, your chances to be in grade A also decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrr.visualize(X, fb, ['revol_util']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As your number of inquiries in past 6 months higher, your chances to be in grade A also decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrr.visualize(X, fb, ['inq_last_6mths']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. What is the minimal improvement need to be done to get grade A from grade G using Constrastive Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "leAG = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainFinalAG = X_trainFinal.drop('id', axis=1).copy()\n",
    "X_trainFinalAG['grade'] = le.inverse_transform(y_train)\n",
    "X_trainFinalAG = X_trainFinalAG[X_trainFinalAG['grade'].isin(['A','G'])]\n",
    "y_trainFinalAG = leAG.fit_transform(X_trainFinalAG['grade'])\n",
    "X_trainFinalAG = X_trainFinalAG.drop('grade', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testFinalAG = X_testFinal.drop('id', axis=1).copy()\n",
    "X_testFinalAG['grade'] = le.inverse_transform(y_test)\n",
    "X_testFinalAG = X_testFinalAG[X_testFinalAG['grade'].isin(['A','G'])]\n",
    "y_testFinalAG = leAG.fit_transform(X_testFinalAG['grade'])\n",
    "X_testFinalAG = X_testFinalAG.drop('grade', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (-0.5,0.5))\n",
    "X_trainFinalScaled = scaler.fit_transform(X_trainFinalAG)\n",
    "X_testFinalScaled = scaler.fit_transform(X_testFinalAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from aix360.algorithms.contrastive import CEMExplainer, KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_small(num_input):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=num_input, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'G'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leAG.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                288       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "class_names = list(leAG.classes_)\n",
    "\n",
    "nn = nn_small(X_trainFinalScaled.shape[1])\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/100\n",
      "6008/6008 [==============================] - 0s 32us/step - loss: 0.6833 - accuracy: 0.7117\n",
      "Epoch 2/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.6502 - accuracy: 0.7921\n",
      "Epoch 3/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.5879 - accuracy: 0.8341\n",
      "Epoch 4/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.5143 - accuracy: 0.8587\n",
      "Epoch 5/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.4445 - accuracy: 0.8790\n",
      "Epoch 6/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.3858 - accuracy: 0.8893\n",
      "Epoch 7/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.3396 - accuracy: 0.8993\n",
      "Epoch 8/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.3021 - accuracy: 0.9050\n",
      "Epoch 9/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.2738 - accuracy: 0.9083\n",
      "Epoch 10/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.2535 - accuracy: 0.9111\n",
      "Epoch 11/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.2388 - accuracy: 0.9126\n",
      "Epoch 12/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.2278 - accuracy: 0.9139\n",
      "Epoch 13/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.2193 - accuracy: 0.9158\n",
      "Epoch 14/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.2126 - accuracy: 0.9168\n",
      "Epoch 15/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.2071 - accuracy: 0.9179\n",
      "Epoch 16/100\n",
      "6008/6008 [==============================] - 0s 12us/step - loss: 0.2025 - accuracy: 0.9199\n",
      "Epoch 17/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1986 - accuracy: 0.9214\n",
      "Epoch 18/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1952 - accuracy: 0.9231\n",
      "Epoch 19/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1923 - accuracy: 0.9231\n",
      "Epoch 20/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1897 - accuracy: 0.9243\n",
      "Epoch 21/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1874 - accuracy: 0.9248\n",
      "Epoch 22/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1854 - accuracy: 0.9248\n",
      "Epoch 23/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1835 - accuracy: 0.9261\n",
      "Epoch 24/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1819 - accuracy: 0.9263\n",
      "Epoch 25/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1804 - accuracy: 0.9263\n",
      "Epoch 26/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1791 - accuracy: 0.9269\n",
      "Epoch 27/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1779 - accuracy: 0.9276\n",
      "Epoch 28/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1768 - accuracy: 0.9288\n",
      "Epoch 29/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1758 - accuracy: 0.9288\n",
      "Epoch 30/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1749 - accuracy: 0.9288\n",
      "Epoch 31/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1741 - accuracy: 0.9288\n",
      "Epoch 32/100\n",
      "6008/6008 [==============================] - 0s 12us/step - loss: 0.1733 - accuracy: 0.9291\n",
      "Epoch 33/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.1726 - accuracy: 0.9293\n",
      "Epoch 34/100\n",
      "6008/6008 [==============================] - 0s 12us/step - loss: 0.1720 - accuracy: 0.9298\n",
      "Epoch 35/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1715 - accuracy: 0.9299\n",
      "Epoch 36/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1710 - accuracy: 0.9301\n",
      "Epoch 37/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1705 - accuracy: 0.9306\n",
      "Epoch 38/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1701 - accuracy: 0.9313\n",
      "Epoch 39/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1697 - accuracy: 0.9316\n",
      "Epoch 40/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1693 - accuracy: 0.9324\n",
      "Epoch 41/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.1690 - accuracy: 0.9319\n",
      "Epoch 42/100\n",
      "6008/6008 [==============================] - 0s 12us/step - loss: 0.1687 - accuracy: 0.9319\n",
      "Epoch 43/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.1684 - accuracy: 0.9324\n",
      "Epoch 44/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1681 - accuracy: 0.9328\n",
      "Epoch 45/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1679 - accuracy: 0.9333\n",
      "Epoch 46/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1677 - accuracy: 0.9331\n",
      "Epoch 47/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1674 - accuracy: 0.9333\n",
      "Epoch 48/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1672 - accuracy: 0.9333\n",
      "Epoch 49/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.1670 - accuracy: 0.9336\n",
      "Epoch 50/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.1668 - accuracy: 0.9339\n",
      "Epoch 51/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1667 - accuracy: 0.9336\n",
      "Epoch 52/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1665 - accuracy: 0.9334\n",
      "Epoch 53/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1663 - accuracy: 0.9334\n",
      "Epoch 54/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1662 - accuracy: 0.9334\n",
      "Epoch 55/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1660 - accuracy: 0.9334\n",
      "Epoch 56/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1659 - accuracy: 0.9333\n",
      "Epoch 57/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1657 - accuracy: 0.9331\n",
      "Epoch 58/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1656 - accuracy: 0.9333\n",
      "Epoch 59/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1655 - accuracy: 0.9331\n",
      "Epoch 60/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1654 - accuracy: 0.9334\n",
      "Epoch 61/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1652 - accuracy: 0.9333\n",
      "Epoch 62/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1651 - accuracy: 0.9331\n",
      "Epoch 63/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1650 - accuracy: 0.9329\n",
      "Epoch 64/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1649 - accuracy: 0.9328\n",
      "Epoch 65/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1648 - accuracy: 0.9329\n",
      "Epoch 66/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1646 - accuracy: 0.9333\n",
      "Epoch 67/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1645 - accuracy: 0.9331\n",
      "Epoch 68/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1645 - accuracy: 0.9331\n",
      "Epoch 69/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1644 - accuracy: 0.9328\n",
      "Epoch 70/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1643 - accuracy: 0.9324\n",
      "Epoch 71/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1642 - accuracy: 0.9324\n",
      "Epoch 72/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1641 - accuracy: 0.9326\n",
      "Epoch 73/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1640 - accuracy: 0.9326\n",
      "Epoch 74/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1639 - accuracy: 0.9324\n",
      "Epoch 75/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1638 - accuracy: 0.9324\n",
      "Epoch 76/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1637 - accuracy: 0.9326\n",
      "Epoch 77/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1636 - accuracy: 0.9323\n",
      "Epoch 78/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1634 - accuracy: 0.9319\n",
      "Epoch 79/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1633 - accuracy: 0.9319\n",
      "Epoch 80/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1632 - accuracy: 0.9319\n",
      "Epoch 81/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1631 - accuracy: 0.9323\n",
      "Epoch 82/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.1630 - accuracy: 0.9323\n",
      "Epoch 83/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1629 - accuracy: 0.9324\n",
      "Epoch 84/100\n",
      "6008/6008 [==============================] - 0s 12us/step - loss: 0.1628 - accuracy: 0.9326\n",
      "Epoch 85/100\n",
      "6008/6008 [==============================] - 0s 14us/step - loss: 0.1628 - accuracy: 0.9324\n",
      "Epoch 86/100\n",
      "6008/6008 [==============================] - 0s 12us/step - loss: 0.1627 - accuracy: 0.9324\n",
      "Epoch 87/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1626 - accuracy: 0.9324\n",
      "Epoch 88/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1625 - accuracy: 0.9328\n",
      "Epoch 89/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1624 - accuracy: 0.9328\n",
      "Epoch 90/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.1624 - accuracy: 0.9326\n",
      "Epoch 91/100\n",
      "6008/6008 [==============================] - 0s 13us/step - loss: 0.1623 - accuracy: 0.9329\n",
      "Epoch 92/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.1622 - accuracy: 0.9328\n",
      "Epoch 93/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1621 - accuracy: 0.9329\n",
      "Epoch 94/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1621 - accuracy: 0.9331\n",
      "Epoch 95/100\n",
      "6008/6008 [==============================] - 0s 12us/step - loss: 0.1620 - accuracy: 0.9333\n",
      "Epoch 96/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.1619 - accuracy: 0.9336\n",
      "Epoch 97/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1619 - accuracy: 0.9336\n",
      "Epoch 98/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.1618 - accuracy: 0.9338\n",
      "Epoch 99/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1617 - accuracy: 0.9338\n",
      "Epoch 100/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.1617 - accuracy: 0.9338\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_trainFinalScaled, y_trainFinalAG, batch_size=128, epochs=100, verbose=1, shuffle=False)\n",
    "nn.save_weights('loan_grade_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9342543482780457\n",
      "Test accuracy: 0.9330669045448303\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy        \n",
    "score = nn.evaluate(X_trainFinalScaled, y_trainFinalAG, verbose=0) #Compute training set accuracy\n",
    "#print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = nn.evaluate(X_testFinalScaled, y_testFinalAG, verbose=0) #Compute test set accuracy\n",
    "#print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainPredicted = nn.predict_classes(X_trainFinalScaled)\n",
    "\n",
    "y_testPredicted = nn.predict_classes(X_testFinalScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAG = pd.DataFrame(scaler.inverse_transform(X_testFinalScaled), columns=X_testFinal.drop('id', axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ariff/.local/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:289: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "dfAG['grade'] = leAG.inverse_transform(y_testFinalAG)\n",
    "dfAG['predicted_grade'] = leAG.inverse_transform(y_testPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testFinalSave = X_testFinal.loc[X_testFinalAG.index, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Get one customer predicted as Grade G to check what he/she need to do to get to grade A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientG = dfAG[dfAG['predicted_grade'] == 'G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_type</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>term</th>\n",
       "      <th>dti</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>...</th>\n",
       "      <th>car</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>debt_consolidation</th>\n",
       "      <th>house</th>\n",
       "      <th>medical</th>\n",
       "      <th>other</th>\n",
       "      <th>personal</th>\n",
       "      <th>small_business</th>\n",
       "      <th>grade</th>\n",
       "      <th>predicted_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>20400.0</td>\n",
       "      <td>58000.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>19.02</td>\n",
       "      <td>2263.0</td>\n",
       "      <td>83.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>84000.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>32.19</td>\n",
       "      <td>10703.0</td>\n",
       "      <td>56.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17250.0</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>27.03</td>\n",
       "      <td>17003.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>745.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.37</td>\n",
       "      <td>6204.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>15725.0</td>\n",
       "      <td>52460.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>15.10</td>\n",
       "      <td>6739.0</td>\n",
       "      <td>57.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>G</td>\n",
       "      <td>G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   application_type  loan_amnt  annual_inc  fico_range_low  term    dti  \\\n",
       "0               2.0    20400.0     58000.0           695.0  60.0  19.02   \n",
       "2               2.0    10000.0     84000.0           670.0  60.0  32.19   \n",
       "6               2.0    17250.0     52000.0           705.0  60.0  27.03   \n",
       "7               2.0    35000.0     80000.0           745.0  60.0  10.37   \n",
       "8               2.0    15725.0     52460.0           665.0  36.0  15.10   \n",
       "\n",
       "   revol_bal  revol_util  inq_last_6mths  delinq_2yrs  ...  car  credit_card  \\\n",
       "0     2263.0        83.8             0.0          0.0  ...  0.0          0.0   \n",
       "2    10703.0        56.6             1.0          0.0  ...  0.0          0.0   \n",
       "6    17003.0        65.0             2.0          0.0  ...  0.0          0.0   \n",
       "7     6204.0        19.4             1.0          0.0  ...  0.0          0.0   \n",
       "8     6739.0        57.1             0.0          0.0  ...  0.0          0.0   \n",
       "\n",
       "   debt_consolidation  house  medical  other  personal  small_business  grade  \\\n",
       "0                 0.0    0.0      0.0    1.0       0.0             0.0      G   \n",
       "2                 1.0    0.0      0.0    0.0       0.0             0.0      G   \n",
       "6                 1.0    0.0      0.0    0.0       0.0             0.0      G   \n",
       "7                 0.0    0.0      0.0    1.0       0.0             0.0      G   \n",
       "8                 1.0    0.0      0.0    0.0       0.0             0.0      G   \n",
       "\n",
       "   predicted_grade  \n",
       "0                G  \n",
       "2                G  \n",
       "6                G  \n",
       "7                G  \n",
       "8                G  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientG.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientG0 = X_testFinalScaled[idx,:].reshape((1,) + X_testFinalScaled[idx,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5       , -0.0025641 , -0.45444338, -0.31081081,  0.5       ,\n",
       "        -0.46424812, -0.49360666,  0.23187773, -0.5       , -0.5       ,\n",
       "        -0.5       , -0.4809264 , -0.49388846, -0.48697366, -0.48866067,\n",
       "        -0.5       , -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "         0.5       , -0.5       , -0.5       ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientG0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/aix360/algorithms/contrastive/CEM_aen.py:54: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/aix360/algorithms/contrastive/CEM_aen.py:86: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/aix360/algorithms/contrastive/CEM_aen.py:150: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/aix360/algorithms/contrastive/CEM_aen.py:153: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/aix360/algorithms/contrastive/CEM_aen.py:167: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "iter:0 const:[100.]\n",
      "Loss_Overall:1000101.8750, Loss_Attack:1000101.8750\n",
      "Loss_L2Dist:0.0000, Loss_L1Dist:0.0000, AE_loss:0.0\n",
      "target_lab_score:0.9988, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[100.]\n",
      "Loss_Overall:1000101.8750, Loss_Attack:1000101.8750\n",
      "Loss_L2Dist:0.0000, Loss_L1Dist:0.0000, AE_loss:0.0\n",
      "target_lab_score:0.9988, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[1000.]\n",
      "Loss_Overall:10001014.0000, Loss_Attack:10001014.0000\n",
      "Loss_L2Dist:0.0119, Loss_L1Dist:0.1581, AE_loss:0.0\n",
      "target_lab_score:0.9942, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1000.]\n",
      "Loss_Overall:10000023.0000, Loss_Attack:10000022.0000\n",
      "Loss_L2Dist:1.1084, Loss_L1Dist:1.4824, AE_loss:0.0\n",
      "target_lab_score:0.0026, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[10000.]\n",
      "Loss_Overall:100000208.0000, Loss_Attack:100000208.0000\n",
      "Loss_L2Dist:1.3870, Loss_L1Dist:2.4384, AE_loss:0.0\n",
      "target_lab_score:0.0011, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[10000.]\n",
      "Loss_Overall:100000192.0000, Loss_Attack:100000192.0000\n",
      "Loss_L2Dist:2.2622, Loss_L1Dist:2.6898, AE_loss:0.0\n",
      "target_lab_score:0.0004, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[100000.]\n",
      "Loss_Overall:1000001984.0000, Loss_Attack:1000001984.0000\n",
      "Loss_L2Dist:5.6407, Loss_L1Dist:6.3546, AE_loss:0.0\n",
      "target_lab_score:0.0001, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[100000.]\n",
      "Loss_Overall:1000001984.0000, Loss_Attack:1000001984.0000\n",
      "Loss_L2Dist:3.7532, Loss_L1Dist:4.1547, AE_loss:0.0\n",
      "target_lab_score:0.0001, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[1000000.]\n",
      "Loss_Overall:10000019456.0000, Loss_Attack:10000019456.0000\n",
      "Loss_L2Dist:8.2953, Loss_L1Dist:8.6229, AE_loss:0.0\n",
      "target_lab_score:0.0002, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1000000.]\n",
      "Loss_Overall:10000019456.0000, Loss_Attack:10000019456.0000\n",
      "Loss_L2Dist:4.5968, Loss_L1Dist:4.7803, AE_loss:0.0\n",
      "target_lab_score:0.0001, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[10000000.]\n",
      "Loss_Overall:100000194560.0000, Loss_Attack:100000194560.0000\n",
      "Loss_L2Dist:8.4950, Loss_L1Dist:8.7284, AE_loss:0.0\n",
      "target_lab_score:0.0002, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[10000000.]\n",
      "Loss_Overall:100000194560.0000, Loss_Attack:100000194560.0000\n",
      "Loss_L2Dist:4.7039, Loss_L1Dist:5.1076, AE_loss:0.0\n",
      "target_lab_score:0.0001, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[1.e+08]\n",
      "Loss_Overall:1000001961984.0000, Loss_Attack:1000001961984.0000\n",
      "Loss_L2Dist:8.4950, Loss_L1Dist:8.7284, AE_loss:0.0\n",
      "target_lab_score:0.0002, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1.e+08]\n",
      "Loss_Overall:1000001961984.0000, Loss_Attack:1000001961984.0000\n",
      "Loss_L2Dist:5.5968, Loss_L1Dist:5.7803, AE_loss:0.0\n",
      "target_lab_score:0.0001, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[1.e+09]\n",
      "Loss_Overall:10000019750912.0000, Loss_Attack:10000019750912.0000\n",
      "Loss_L2Dist:8.4950, Loss_L1Dist:8.7284, AE_loss:0.0\n",
      "target_lab_score:0.0002, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1.e+09]\n",
      "Loss_Overall:10000019750912.0000, Loss_Attack:10000019750912.0000\n",
      "Loss_L2Dist:5.5968, Loss_L1Dist:5.7803, AE_loss:0.0\n",
      "target_lab_score:0.0001, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[1.e+10]\n",
      "Loss_Overall:100000193314816.0000, Loss_Attack:100000193314816.0000\n",
      "Loss_L2Dist:8.4950, Loss_L1Dist:8.7284, AE_loss:0.0\n",
      "target_lab_score:0.0002, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1.e+10]\n",
      "Loss_Overall:100000193314816.0000, Loss_Attack:100000193314816.0000\n",
      "Loss_L2Dist:5.5968, Loss_L1Dist:5.7803, AE_loss:0.0\n",
      "target_lab_score:0.0001, max_nontarget_lab_score:-10000.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PN' # Find pertinent negatives\n",
    "arg_max_iter = 500 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 100 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.02 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 0.03 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 1 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "\n",
    "# Find PN for applicant 1272\n",
    "(adv_pn, delta_pn, info_pn) = explainer.explain_instance(clientG0, arg_mode, my_AE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A'], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leAG.inverse_transform(nn.predict_classes(adv_pn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclientA = pd.DataFrame(scaler.inverse_transform(adv_pn), columns=X_testFinalAG.columns).T\n",
    "dfclientA.rename(columns={0:'A'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclientSelected = pd.DataFrame(scaler.inverse_transform(clientG0), columns=X_testFinalAG.columns).T\n",
    "dfclientSelected.rename(columns={0:'G'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffASelected = dfclientA['A'] - dfclientSelected['G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffASelected = pd.DataFrame(diffASelected, columns=['Different'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G</th>\n",
       "      <th>A</th>\n",
       "      <th>Different</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>application_type</th>\n",
       "      <td>2.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>20400.00</td>\n",
       "      <td>20500.00</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_inc</th>\n",
       "      <td>58000.00</td>\n",
       "      <td>624600.00</td>\n",
       "      <td>566600.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fico_range_low</th>\n",
       "      <td>695.00</td>\n",
       "      <td>752.50</td>\n",
       "      <td>57.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <td>60.00</td>\n",
       "      <td>48.00</td>\n",
       "      <td>-12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dti</th>\n",
       "      <td>19.02</td>\n",
       "      <td>266.00</td>\n",
       "      <td>246.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revol_bal</th>\n",
       "      <td>2263.00</td>\n",
       "      <td>176981.00</td>\n",
       "      <td>174718.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revol_util</th>\n",
       "      <td>83.80</td>\n",
       "      <td>57.25</td>\n",
       "      <td>-26.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_rec</th>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_bal_il</th>\n",
       "      <td>11440.00</td>\n",
       "      <td>299891.00</td>\n",
       "      <td>288451.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <td>2700.00</td>\n",
       "      <td>196650.00</td>\n",
       "      <td>193950.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <td>2284.00</td>\n",
       "      <td>87668.50</td>\n",
       "      <td>85384.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <td>26112.00</td>\n",
       "      <td>996251.50</td>\n",
       "      <td>970139.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit_card</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt_consolidation</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small_business</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           G          A  Different\n",
       "application_type        2.00       2.50       0.50\n",
       "loan_amnt           20400.00   20500.00     100.00\n",
       "annual_inc          58000.00  624600.00  566600.00\n",
       "fico_range_low        695.00     752.50      57.50\n",
       "term                   60.00      48.00     -12.00\n",
       "dti                    19.02     266.00     246.98\n",
       "revol_bal            2263.00  176981.00  174718.00\n",
       "revol_util             83.80      57.25     -26.55\n",
       "inq_last_6mths          0.00       2.50       2.50\n",
       "delinq_2yrs             0.00       6.00       6.00\n",
       "pub_rec                 0.00       3.00       3.00\n",
       "total_bal_il        11440.00  299891.00  288451.00\n",
       "total_rev_hi_lim     2700.00  196650.00  193950.00\n",
       "avg_cur_bal          2284.00   87668.50   85384.50\n",
       "tot_hi_cred_lim     26112.00  996251.50  970139.50\n",
       "car                     0.00       0.50       0.50\n",
       "credit_card             0.00       0.50       0.50\n",
       "debt_consolidation      0.00       0.50       0.50\n",
       "house                   0.00       0.50       0.50\n",
       "medical                 0.00       0.50       0.50\n",
       "other                   1.00       0.50      -0.50\n",
       "personal                0.00       0.50       0.50\n",
       "small_business          0.00       0.50       0.50"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([dfclientSelected, dfclientA, diffASelected], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement needed are to be in grade G from A:\n",
    "\n",
    "- less the loan term, at max 48 months\n",
    "- lower you revolving utilization rate\n",
    "- increase your annual income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get one customer predicted as Grade C to check what he/she need to do to get to grade B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "leBC = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainFinalBC = X_trainFinal.copy()\n",
    "X_trainFinalBC['grade'] = le.inverse_transform(y_train)\n",
    "X_trainFinalBC = X_trainFinalBC[X_trainFinalBC['grade'].isin(['B','C'])]\n",
    "y_trainFinalBC = leBC.fit_transform(X_trainFinalBC['grade'])\n",
    "X_trainFinalBC = X_trainFinalBC.drop('grade', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testFinalBC = X_testFinal.copy()\n",
    "X_testFinalBC['grade'] = le.inverse_transform(y_test)\n",
    "X_testFinalBC = X_testFinalBC[X_testFinalBC['grade'].isin(['B','C'])]\n",
    "y_testFinalBC = leBC.fit_transform(X_testFinalBC['grade'])\n",
    "X_testFinalBC = X_testFinalBC.drop('grade', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range = (-0.5,0.5))\n",
    "X_trainFinalScaled = scaler.fit_transform(X_trainFinalBC)\n",
    "X_testFinalScaled = scaler.fit_transform(X_testFinalBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aix360.algorithms.contrastive import CEMExplainer, KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_small(num_input):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=num_input, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B', 'C'], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leBC.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 313\n",
      "Trainable params: 313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "class_names = list(leBC.classes_)\n",
    "\n",
    "nn = nn_small(X_trainFinalScaled.shape[1])\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter-ariff/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "Epoch 1/100\n",
      "6008/6008 [==============================] - 0s 30us/step - loss: 0.6921 - accuracy: 0.5183\n",
      "Epoch 2/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6887 - accuracy: 0.5652\n",
      "Epoch 3/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6836 - accuracy: 0.5774\n",
      "Epoch 4/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6772 - accuracy: 0.5904\n",
      "Epoch 5/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6709 - accuracy: 0.5942\n",
      "Epoch 6/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6654 - accuracy: 0.5982\n",
      "Epoch 7/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6607 - accuracy: 0.6034\n",
      "Epoch 8/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6567 - accuracy: 0.6092\n",
      "Epoch 9/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6534 - accuracy: 0.6128\n",
      "Epoch 10/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6506 - accuracy: 0.6167\n",
      "Epoch 11/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6483 - accuracy: 0.6185\n",
      "Epoch 12/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6464 - accuracy: 0.6195\n",
      "Epoch 13/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6447 - accuracy: 0.6200\n",
      "Epoch 14/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6434 - accuracy: 0.6215\n",
      "Epoch 15/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6422 - accuracy: 0.6223\n",
      "Epoch 16/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6412 - accuracy: 0.6245\n",
      "Epoch 17/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6404 - accuracy: 0.6258\n",
      "Epoch 18/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6397 - accuracy: 0.6258\n",
      "Epoch 19/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6390 - accuracy: 0.6277\n",
      "Epoch 20/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6384 - accuracy: 0.6295\n",
      "Epoch 21/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6379 - accuracy: 0.6312\n",
      "Epoch 22/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6374 - accuracy: 0.6318\n",
      "Epoch 23/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6370 - accuracy: 0.6315\n",
      "Epoch 24/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6365 - accuracy: 0.6347\n",
      "Epoch 25/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6361 - accuracy: 0.6347\n",
      "Epoch 26/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6357 - accuracy: 0.6352\n",
      "Epoch 27/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6354 - accuracy: 0.6352\n",
      "Epoch 28/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6350 - accuracy: 0.6368\n",
      "Epoch 29/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6347 - accuracy: 0.6368\n",
      "Epoch 30/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6344 - accuracy: 0.6376\n",
      "Epoch 31/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6341 - accuracy: 0.6386\n",
      "Epoch 32/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.6339 - accuracy: 0.6390\n",
      "Epoch 33/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6336 - accuracy: 0.6396\n",
      "Epoch 34/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6334 - accuracy: 0.6398\n",
      "Epoch 35/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6331 - accuracy: 0.6405\n",
      "Epoch 36/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6329 - accuracy: 0.6401\n",
      "Epoch 37/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6326 - accuracy: 0.6401\n",
      "Epoch 38/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6323 - accuracy: 0.6403\n",
      "Epoch 39/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6321 - accuracy: 0.6395\n",
      "Epoch 40/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6318 - accuracy: 0.6383\n",
      "Epoch 41/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6316 - accuracy: 0.6390\n",
      "Epoch 42/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6313 - accuracy: 0.6391\n",
      "Epoch 43/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6311 - accuracy: 0.6398\n",
      "Epoch 44/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6309 - accuracy: 0.6406\n",
      "Epoch 45/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6306 - accuracy: 0.6398\n",
      "Epoch 46/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6304 - accuracy: 0.6405\n",
      "Epoch 47/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6301 - accuracy: 0.6405\n",
      "Epoch 48/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6298 - accuracy: 0.6391\n",
      "Epoch 49/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6295 - accuracy: 0.6406\n",
      "Epoch 50/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6292 - accuracy: 0.6406\n",
      "Epoch 51/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6290 - accuracy: 0.6413\n",
      "Epoch 52/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6288 - accuracy: 0.6410\n",
      "Epoch 53/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6286 - accuracy: 0.6401\n",
      "Epoch 54/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.6284 - accuracy: 0.6408\n",
      "Epoch 55/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6282 - accuracy: 0.6406\n",
      "Epoch 56/100\n",
      "6008/6008 [==============================] - ETA: 0s - loss: 0.6305 - accuracy: 0.63 - 0s 11us/step - loss: 0.6281 - accuracy: 0.6406\n",
      "Epoch 57/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6279 - accuracy: 0.6413\n",
      "Epoch 58/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6277 - accuracy: 0.6418\n",
      "Epoch 59/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.6276 - accuracy: 0.6410\n",
      "Epoch 60/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6274 - accuracy: 0.6416\n",
      "Epoch 61/100\n",
      "6008/6008 [==============================] - ETA: 0s - loss: 0.6283 - accuracy: 0.63 - 0s 11us/step - loss: 0.6273 - accuracy: 0.6413\n",
      "Epoch 62/100\n",
      "6008/6008 [==============================] - 0s 11us/step - loss: 0.6271 - accuracy: 0.6408\n",
      "Epoch 63/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6270 - accuracy: 0.6406\n",
      "Epoch 64/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6269 - accuracy: 0.6408\n",
      "Epoch 65/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6268 - accuracy: 0.6408\n",
      "Epoch 66/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6266 - accuracy: 0.6406\n",
      "Epoch 67/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6265 - accuracy: 0.6406\n",
      "Epoch 68/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6264 - accuracy: 0.6410\n",
      "Epoch 69/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6263 - accuracy: 0.6405\n",
      "Epoch 70/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6261 - accuracy: 0.6400\n",
      "Epoch 71/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6260 - accuracy: 0.6396\n",
      "Epoch 72/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6259 - accuracy: 0.6401\n",
      "Epoch 73/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6258 - accuracy: 0.6408\n",
      "Epoch 74/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6257 - accuracy: 0.6411\n",
      "Epoch 75/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6256 - accuracy: 0.6415\n",
      "Epoch 76/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6255 - accuracy: 0.6418\n",
      "Epoch 77/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6254 - accuracy: 0.6415\n",
      "Epoch 78/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6253 - accuracy: 0.6418\n",
      "Epoch 79/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6252 - accuracy: 0.6423\n",
      "Epoch 80/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6251 - accuracy: 0.6423\n",
      "Epoch 81/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6250 - accuracy: 0.6425\n",
      "Epoch 82/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6249 - accuracy: 0.6435\n",
      "Epoch 83/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6248 - accuracy: 0.6436\n",
      "Epoch 84/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6248 - accuracy: 0.6440\n",
      "Epoch 85/100\n",
      "6008/6008 [==============================] - 0s 12us/step - loss: 0.6247 - accuracy: 0.6435\n",
      "Epoch 86/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6246 - accuracy: 0.6433\n",
      "Epoch 87/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6245 - accuracy: 0.6436\n",
      "Epoch 88/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6245 - accuracy: 0.6441\n",
      "Epoch 89/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6244 - accuracy: 0.6438\n",
      "Epoch 90/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6243 - accuracy: 0.6440\n",
      "Epoch 91/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6243 - accuracy: 0.6440\n",
      "Epoch 92/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6242 - accuracy: 0.6441\n",
      "Epoch 93/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6241 - accuracy: 0.6438\n",
      "Epoch 94/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6240 - accuracy: 0.6438\n",
      "Epoch 95/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6240 - accuracy: 0.6433\n",
      "Epoch 96/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6239 - accuracy: 0.6431\n",
      "Epoch 97/100\n",
      "6008/6008 [==============================] - 0s 10us/step - loss: 0.6239 - accuracy: 0.6438\n",
      "Epoch 98/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6238 - accuracy: 0.6438\n",
      "Epoch 99/100\n",
      "6008/6008 [==============================] - 0s 9us/step - loss: 0.6237 - accuracy: 0.6436\n",
      "Epoch 100/100\n",
      "6008/6008 [==============================] - 0s 8us/step - loss: 0.6237 - accuracy: 0.6445\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_trainFinalScaled, y_trainFinalBC, batch_size=128, epochs=100, verbose=1, shuffle=False)\n",
    "nn.save_weights('loan_grade_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6446405053138733\n",
      "Test accuracy: 0.6258741021156311\n"
     ]
    }
   ],
   "source": [
    "# evaluate model accuracy        \n",
    "score = nn.evaluate(X_trainFinalScaled, y_trainFinalBC, verbose=0) #Compute training set accuracy\n",
    "#print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = nn.evaluate(X_testFinalScaled, y_testFinalBC, verbose=0) #Compute test set accuracy\n",
    "#print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainPredicted = nn.predict_classes(X_trainFinalScaled)\n",
    "\n",
    "y_testPredicted = nn.predict_classes(X_testFinalScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfBC = pd.DataFrame(scaler.inverse_transform(X_testFinalScaled), columns=X_testFinal.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-ariff/.local/lib/python3.7/site-packages/sklearn/preprocessing/_label.py:289: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "dfBC['grade'] = leBC.inverse_transform(y_testFinalBC)\n",
    "dfBC['predicted_grade'] = leBC.inverse_transform(y_testPredicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientC = dfBC[dfBC['predicted_grade'] == 'C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>application_type</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>fico_range_low</th>\n",
       "      <th>term</th>\n",
       "      <th>dti</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>...</th>\n",
       "      <th>car</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>debt_consolidation</th>\n",
       "      <th>house</th>\n",
       "      <th>medical</th>\n",
       "      <th>other</th>\n",
       "      <th>personal</th>\n",
       "      <th>small_business</th>\n",
       "      <th>grade</th>\n",
       "      <th>predicted_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108106895.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>152000.0</td>\n",
       "      <td>695.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>16.30</td>\n",
       "      <td>23069.0</td>\n",
       "      <td>52.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>139156966.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>26.45</td>\n",
       "      <td>7202.0</td>\n",
       "      <td>43.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>75230741.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10.58</td>\n",
       "      <td>5772.0</td>\n",
       "      <td>25.9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>126788231.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>97000.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>5643.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>130806291.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>20455.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>112.50</td>\n",
       "      <td>31769.0</td>\n",
       "      <td>74.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  application_type  loan_amnt  annual_inc  fico_range_low  term  \\\n",
       "3  108106895.0               2.0    14000.0    152000.0           695.0  60.0   \n",
       "4  139156966.0               2.0    10000.0     45000.0           705.0  60.0   \n",
       "5   75230741.0               2.0    12000.0     45000.0           660.0  36.0   \n",
       "7  126788231.0               2.0    15000.0     97000.0           670.0  60.0   \n",
       "8  130806291.0               2.0    40000.0     20455.0           705.0  60.0   \n",
       "\n",
       "      dti  revol_bal  revol_util  inq_last_6mths  ...  car  credit_card  \\\n",
       "3   16.30    23069.0        52.8             0.0  ...  0.0          0.0   \n",
       "4   26.45     7202.0        43.4             0.0  ...  0.0          0.0   \n",
       "5   10.58     5772.0        25.9             4.0  ...  0.0          0.0   \n",
       "7    6.00     5643.0        20.4             0.0  ...  0.0          0.0   \n",
       "8  112.50    31769.0        74.8             0.0  ...  0.0          0.0   \n",
       "\n",
       "   debt_consolidation  house  medical  other  personal  small_business  grade  \\\n",
       "3                 1.0    0.0      0.0    0.0       0.0             0.0      C   \n",
       "4                 1.0    0.0      0.0    0.0       0.0             0.0      B   \n",
       "5                 1.0    0.0      0.0    0.0       0.0             0.0      C   \n",
       "7                 1.0    0.0      0.0    0.0       0.0             0.0      C   \n",
       "8                 1.0    0.0      0.0    0.0       0.0             0.0      B   \n",
       "\n",
       "   predicted_grade  \n",
       "3                C  \n",
       "4                C  \n",
       "5                C  \n",
       "7                C  \n",
       "8                C  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    1.3749e+08\n",
       "application_type               2\n",
       "loan_amnt                  20000\n",
       "annual_inc                 54000\n",
       "fico_range_low               665\n",
       "term                          36\n",
       "dti                         7.62\n",
       "revol_bal                   8579\n",
       "revol_util                  11.3\n",
       "inq_last_6mths                 3\n",
       "delinq_2yrs                    0\n",
       "pub_rec                        1\n",
       "total_bal_il                2687\n",
       "total_rev_hi_lim           76200\n",
       "avg_cur_bal                  433\n",
       "tot_hi_cred_lim            82054\n",
       "car                            0\n",
       "credit_card                    0\n",
       "debt_consolidation             1\n",
       "house                          0\n",
       "medical                        0\n",
       "other                          0\n",
       "personal                       0\n",
       "small_business                 0\n",
       "grade                          B\n",
       "predicted_grade                C\n",
       "Name: 18, dtype: object"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clientC.iloc[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientC0 = X_testFinalScaled[idx,:].reshape((1,) + X_testFinalScaled[idx,:].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 const:[1.]\n",
      "Loss_Overall:10000.8799, Loss_Attack:10000.8799\n",
      "Loss_L2Dist:0.0000, Loss_L1Dist:0.0000, AE_loss:0.0\n",
      "target_lab_score:0.8608, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1.]\n",
      "Loss_Overall:10000.8799, Loss_Attack:10000.8799\n",
      "Loss_L2Dist:0.0000, Loss_L1Dist:0.0000, AE_loss:0.0\n",
      "target_lab_score:0.8608, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[10.]\n",
      "Loss_Overall:100008.6797, Loss_Attack:100008.6797\n",
      "Loss_L2Dist:0.0005, Loss_L1Dist:0.0228, AE_loss:0.0\n",
      "target_lab_score:0.8483, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[10.]\n",
      "Loss_Overall:100003.6250, Loss_Attack:100003.1875\n",
      "Loss_L2Dist:0.4232, Loss_L1Dist:0.6505, AE_loss:0.0\n",
      "target_lab_score:0.2989, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[100.]\n",
      "Loss_Overall:1000025.1250, Loss_Attack:1000024.6875\n",
      "Loss_L2Dist:0.3615, Loss_L1Dist:1.2148, AE_loss:0.0\n",
      "target_lab_score:0.2278, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[100.]\n",
      "Loss_Overall:1000011.1250, Loss_Attack:1000009.5625\n",
      "Loss_L2Dist:1.4744, Loss_L1Dist:2.2625, AE_loss:0.0\n",
      "target_lab_score:0.0764, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[1000.]\n",
      "Loss_Overall:10000049.0000, Loss_Attack:10000044.0000\n",
      "Loss_L2Dist:5.3296, Loss_L1Dist:6.9141, AE_loss:0.0\n",
      "target_lab_score:0.0248, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1000.]\n",
      "Loss_Overall:10000054.0000, Loss_Attack:10000050.0000\n",
      "Loss_L2Dist:3.7016, Loss_L1Dist:3.8461, AE_loss:0.0\n",
      "target_lab_score:0.0303, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[10000.]\n",
      "Loss_Overall:100000464.0000, Loss_Attack:100000456.0000\n",
      "Loss_L2Dist:9.6138, Loss_L1Dist:9.9842, AE_loss:0.0\n",
      "target_lab_score:0.0267, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[10000.]\n",
      "Loss_Overall:100000344.0000, Loss_Attack:100000336.0000\n",
      "Loss_L2Dist:11.3476, Loss_L1Dist:11.6607, AE_loss:0.0\n",
      "target_lab_score:0.0137, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[100000.]\n",
      "Loss_Overall:1000004672.0000, Loss_Attack:1000004672.0000\n",
      "Loss_L2Dist:10.5722, Loss_L1Dist:10.7801, AE_loss:0.0\n",
      "target_lab_score:0.0277, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[100000.]\n",
      "Loss_Overall:1000003328.0000, Loss_Attack:1000003328.0000\n",
      "Loss_L2Dist:11.3772, Loss_L1Dist:11.8329, AE_loss:0.0\n",
      "target_lab_score:0.0136, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[1000000.]\n",
      "Loss_Overall:10000047104.0000, Loss_Attack:10000047104.0000\n",
      "Loss_L2Dist:10.5722, Loss_L1Dist:10.7801, AE_loss:0.0\n",
      "target_lab_score:0.0277, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1000000.]\n",
      "Loss_Overall:10000032768.0000, Loss_Attack:10000032768.0000\n",
      "Loss_L2Dist:12.3476, Loss_L1Dist:12.6607, AE_loss:0.0\n",
      "target_lab_score:0.0134, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[10000000.]\n",
      "Loss_Overall:100000464896.0000, Loss_Attack:100000464896.0000\n",
      "Loss_L2Dist:10.5722, Loss_L1Dist:10.7801, AE_loss:0.0\n",
      "target_lab_score:0.0277, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[10000000.]\n",
      "Loss_Overall:100000333824.0000, Loss_Attack:100000333824.0000\n",
      "Loss_L2Dist:12.3476, Loss_L1Dist:12.6607, AE_loss:0.0\n",
      "target_lab_score:0.0134, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:0 const:[1.e+08]\n",
      "Loss_Overall:1000004714496.0000, Loss_Attack:1000004714496.0000\n",
      "Loss_L2Dist:10.5722, Loss_L1Dist:10.7801, AE_loss:0.0\n",
      "target_lab_score:0.0277, max_nontarget_lab_score:-10000.0000\n",
      "\n",
      "iter:250 const:[1.e+08]\n",
      "Loss_Overall:1000003338240.0000, Loss_Attack:1000003338240.0000\n",
      "Loss_L2Dist:12.3476, Loss_L1Dist:12.6607, AE_loss:0.0\n",
      "target_lab_score:0.0134, max_nontarget_lab_score:-10000.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PN' # Find pertinent negatives\n",
    "arg_max_iter = 500 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 1 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.02 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 0.03 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 1 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_BE_model = None # Pointer to an auto-encoder\n",
    "\n",
    "# Find PN for applicant 1272\n",
    "(adv_pn, delta_pn, info_pn) = explainer.explain_instance(clientC0, arg_mode, my_BE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B'], dtype=object)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leBC.inverse_transform(nn.predict_classes(adv_pn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclientB = pd.DataFrame(scaler.inverse_transform(adv_pn), columns=X_testFinal.columns).T\n",
    "dfclientB.rename(columns={0:'B'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfclientC0 = pd.DataFrame(scaler.inverse_transform(clientC0), columns=X_testFinal.columns).T\n",
    "dfclientC0.rename(columns={0:'C'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffCB = dfclientB['B'] - dfclientC0['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffCB = pd.DataFrame(diffCB, columns=['Improvement Needed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improvement needed are to be in grade B from C:\n",
    "\n",
    "- less the loan term, at max 48 months\n",
    "- lower you revolving utilization rate\n",
    "- increase your annual income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>B</th>\n",
       "      <th>Improvement Needed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>75061241.00</td>\n",
       "      <td>1.035910e+08</td>\n",
       "      <td>2.852976e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>application_type</th>\n",
       "      <td>2.00</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_amnt</th>\n",
       "      <td>1000.00</td>\n",
       "      <td>2.050000e+04</td>\n",
       "      <td>1.950000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annual_inc</th>\n",
       "      <td>75000.00</td>\n",
       "      <td>7.510000e+05</td>\n",
       "      <td>6.760000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fico_range_low</th>\n",
       "      <td>675.00</td>\n",
       "      <td>7.450000e+02</td>\n",
       "      <td>7.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term</th>\n",
       "      <td>36.00</td>\n",
       "      <td>4.800000e+01</td>\n",
       "      <td>1.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dti</th>\n",
       "      <td>8.91</td>\n",
       "      <td>3.491150e+02</td>\n",
       "      <td>3.402050e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revol_bal</th>\n",
       "      <td>10765.00</td>\n",
       "      <td>3.108715e+05</td>\n",
       "      <td>3.001065e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revol_util</th>\n",
       "      <td>94.00</td>\n",
       "      <td>5.200000e+01</td>\n",
       "      <td>-4.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <td>4.00</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>-1.500000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <td>0.00</td>\n",
       "      <td>7.500000e+00</td>\n",
       "      <td>7.500000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pub_rec</th>\n",
       "      <td>0.00</td>\n",
       "      <td>2.500000e+00</td>\n",
       "      <td>2.500000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_bal_il</th>\n",
       "      <td>15491.00</td>\n",
       "      <td>2.403165e+05</td>\n",
       "      <td>2.248255e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_rev_hi_lim</th>\n",
       "      <td>11500.00</td>\n",
       "      <td>3.627500e+05</td>\n",
       "      <td>3.512500e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <td>3750.00</td>\n",
       "      <td>6.679250e+04</td>\n",
       "      <td>6.304250e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tot_hi_cred_lim</th>\n",
       "      <td>36593.00</td>\n",
       "      <td>8.146035e+05</td>\n",
       "      <td>7.780105e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credit_card</th>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt_consolidation</th>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical</th>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>1.00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal</th>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small_business</th>\n",
       "      <td>0.00</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>5.000000e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              C             B  Improvement Needed\n",
       "id                  75061241.00  1.035910e+08        2.852976e+07\n",
       "application_type           2.00  2.500000e+00        5.000000e-01\n",
       "loan_amnt               1000.00  2.050000e+04        1.950000e+04\n",
       "annual_inc             75000.00  7.510000e+05        6.760000e+05\n",
       "fico_range_low           675.00  7.450000e+02        7.000000e+01\n",
       "term                      36.00  4.800000e+01        1.200000e+01\n",
       "dti                        8.91  3.491150e+02        3.402050e+02\n",
       "revol_bal              10765.00  3.108715e+05        3.001065e+05\n",
       "revol_util                94.00  5.200000e+01       -4.200000e+01\n",
       "inq_last_6mths             4.00  2.500000e+00       -1.500000e+00\n",
       "delinq_2yrs                0.00  7.500000e+00        7.500000e+00\n",
       "pub_rec                    0.00  2.500000e+00        2.500000e+00\n",
       "total_bal_il           15491.00  2.403165e+05        2.248255e+05\n",
       "total_rev_hi_lim       11500.00  3.627500e+05        3.512500e+05\n",
       "avg_cur_bal             3750.00  6.679250e+04        6.304250e+04\n",
       "tot_hi_cred_lim        36593.00  8.146035e+05        7.780105e+05\n",
       "car                        0.00  5.000000e-01        5.000000e-01\n",
       "credit_card                0.00  5.000000e-01        5.000000e-01\n",
       "debt_consolidation         0.00  5.000000e-01        5.000000e-01\n",
       "house                      0.00  5.000000e-01        5.000000e-01\n",
       "medical                    0.00  5.000000e-01        5.000000e-01\n",
       "other                      1.00  5.000000e-01       -5.000000e-01\n",
       "personal                   0.00  5.000000e-01        5.000000e-01\n",
       "small_business             0.00  5.000000e-01        5.000000e-01"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([dfclientC0, dfclientB, diffCB], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Boolean Rule and Logistic Rule Regression models, we able to grasp general knowledge on how we can distinguish between good grade and bad grade creditors. And then we proceed with identifying the improvement needed for future creditor to have higher chances of getting their loan approved by giving them general information such as Income, Term of the loan and their utilization of credit card."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
